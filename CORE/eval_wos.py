from annoy import AnnoyIndex
import random,time,itertools,os,pickle,redis,json
from scipy.misc import comb
import numpy as np
import pandas as pd
import multiprocess as mp
from tqdm import tqdm as tq

help_string="""
THIS CODE IS ONLY TESTED AGAINST PYTHON 3.6!!!

Script for running wos category-based evaluations of a d2v vector space. Could be modified for evaluating under some other "ground truth" cateogry structure. Not that this depends on generating both a redis database to store category data and leverages annoy for fast computation of doc-doc cosine distances. The redis database must be up and running before using this script

Example viz of results in /notebooks/eval.ipynb

Generates three files:

 - acc_df.pkl: a pickled pandas dataframe in the following format (example data):

           a  agree         b         cats_a     cats_b      dist  overlap
0   6401016      0   7424379           [51]   [30, 95]  0.939160      0.0
1  16007793      0  19859798  [35, 37, 141]  [98, 100]  1.184968      0.0
2   3020363      0  15707088          [162]      [152]  0.594797      0.0
3   6216355      0  18060360          [145]       [33]  0.844678      0.0
4  18310420      0  19498095      [46, 146]   [21, 24]  0.653071      0.0

This is generated by randomly sampling many random pairs of documents (i.e. documents `a` and `b`). `cats_a` and `cats_b` are the category label s for each document, `agree` indicates whether at least one category assignment between the two agrees, and `overlap` is the proportion of overlap between the two (see code for details on definition).

 - cat_results.pkl: This is generated by randomly sampling pairs *from within* a category. File stores a dictionary where keys are categories and values are arrays of all the calculated cosine distances between those pairs.

 - cat_results_binned.pkl: The same data as in cat_results, but binned using `np.histogram(cat_results[cat],bins=bins)[0]`. Bins defaults to `np.arange(0,2,.01)`.

"""

bins = np.arange(0,2,.01)

# annoy returns euclidean distance of normed vector
# (i.e. sqrt(2-2*cos(u, v))), so this just converts
# to standard cosine distance
def convert_distance(d):
   return (d**2) /2
vec_convert_distance = np.vectorize(convert_distance)

# computes everything described for acc_df in help string
def random_comps_acc(n):
    pid = os.getpid()
    np.random.seed(int(time.time()/1000)+pid)
    ds = []
    a_ = []
    b_  = []
    cats_a_ = []
    cats_b_ = []
    compset = set()
    first_cat_agreement = []
    cat_overlap = []
    done = 0
    while done<n:
        if done%10000==0:
            print("{}/{} ({})".format(done,n,os.getpid()))
        a = np.random.randint(0,total_docs)
        b = np.random.randint(0,total_docs)
        if a!=b:
            if a>b:
                a,b = b,a
            if (a,b) not in compset:
                try:
                    h_a,sh_a,cats_a = json.loads(r.get(a))
                    nca = len(cats_a)
                    h_b,sh_b,cats_b = json.loads(r.get(b))
                    ncb = len(cats_b)
                except TypeError: # if no result in DB
                    continue
                ds.append(convert_distance(t.get_distance(a,b)))
                compset.add((a,b))
                a_.append(a)
                b_.append(b)
                cats_a_.append(cats_a)
                cats_b_.append(cats_b)
                intersection = len(set(cats_a).intersection(cats_b))
                first_cat_agreement.append(int(intersection>0))
                cat_overlap.append( intersection / min(nca,ncb))
                done += 1
    return pd.DataFrame({'a':a_,'b':b_,'dist':ds,'overlap':cat_overlap,'agree':first_cat_agreement,'cats_a':cats_a_,'cats_b':cats_b_})

# Does all setup of category files and redis database. Only needs to be done once.
def setup_cats(setup_redis=False):
    cat_dict = pickle.load(open('cat_dict.pkl','rb'))
    year_dict = pickle.load(open('year_dict.pkl','rb'))
    cat_indices = {i:[] for i in range(251)}
    cat_year_indices = {}
    for i in range(251):
        cat_year_indices[i] = {y:[] for y in range(1991,2016)}
    for k,v in tq(cat_dict.items()):
        if setup_redis:
            r.set(k,json.dumps(v))
        cats = v[2]
        year = year_dict[k]
        for c in cats:
            try:
                cat_indices[c].append(k)
                cat_year_indices[c][year].append(k)
            except KeyError:
                continue
    for c in tq(cat_indices):
        cat_indices[c] = np.array(cat_indices[c])
    for c in tq(cat_year_indices):
        for y in tq(cat_year_indices[c]):
            cat_year_indices[c][y] = np.array(cat_year_indices[c][y])
    return cat_indices,cat_year_indices

# calculates distances for n random pairs of documents within a given category`
def incat_similarity(tup):
    cat_idx,n = tup
    indices = cat_indices[cat_idx]
    pid = os.getpid()
    np.random.seed(int(time.time()/1000)+pid)
    ds = []
    compset = set()
    done = 0
    while done<n:
        a = np.random.choice(indices)
        b = np.random.choice(indices)
        if a!=b:
            if a>b:
                a,b = b,a
            if (a,b) not in compset:
                ds.append(convert_distance(t.get_distance(a,b)))
                compset.add((a,b))
                done += 1
    return ds


if __name__ == '__main__':

    parser = argparse.ArgumentParser(help_string)
    parser.add_argument("--params", required=True,help="specify d2v model paramter in format 'size-window-min_count-sample-random_seed', e.g. '100-5-5-0-None' (see gensim doc2vec documentation and d2v.py for details of these parameters)",type=str)
    parser.add_argument("--workers", help="Number of workers to use in parallel computations. Defaults to output of mp.cpu_count()",default=mp.cpu_count(),type=int)
    parser.add_argument("--redis", help="Port for locally running redis server. Defaults to redist default of 6379",default=6379,type=int)
    parser.add_argument("--annoy_path", help="Path to annoy index. If file does not exist, a new index will be generated. This should be a global index (see helpstring for lde.py)",default='./index.ann',type=str)
    parser.add_argument("--d2vdir",help="path to doc2vec model directory",default='/backup/home/jared/storage/wos-text-dynamics-data/d2v-wos/',type=str)
    parser.add_argument("--setup",action='store_true',help="If specified, setup the redis database and generate all necessary dictionaries. Otherwise assume this is already done.")
    parser.add_argument("--chunksize",help="Number of random samples to generate per parallel worker. Defaults to 100k",default=100000,type=int)

    args = parser.parse_args()

    r = redis.StrictRedis(host='localhost', port=args.redis, db=0)

    # BUILD ANNOY INDEX

    f = int(args.params.split('-')[0])
    if os.path.exists(args.annoy_path):
        t = AnnoyIndex(f)
        t.load('test.ann')

    else:

        if args.params.split('-')[-1] == 'None':
            features = np.load('{0}{1}/model_{1}.docvecs.doctag_syn0.npy'.format(args.d2vdir,args.params))
        else:
            features = np.load('{0}{1}/doc_features_expanded_{1}.npy'.format(args.d2vdir,args.params))
        for i,vec in tq(enumerate(features)):
            t.add_item(i, vec)
        t.build(50) 
        t.save(args.annoy_path)
        del features

    if args.setup:
        cat_indices,cat_year_indices = setup_cats()
    else:
        cat_indices = pickle.load(open(args.d2vdir+'cat_indices.pkl','rb'))
        cat_year_indices = pickle.load(open(args.d2vdir+'cat_year_indices.pkl','rb'))

    pool = mp.Pool(arg.workers)
    acc =  pool.map(random_comps_acc,itertools.repeat(chunksize,args.workers))
    acc_df = pd.concat(acc)
    pickle.dump(acc_df,open('acc_df.pkl','wb'))
    cat_results = {}
    for cat in tq(range(251)):
        possible_comps = comb(len(cat_indices[cat]),2)
        if possible_comps <= (chunksize*n_procs):
            result = pool.map(lambda a: t.get_distance(a[0],a[1]),itertools.combinations(cat_indices[cat],2))
            cat_results[cat] = np.array(result)
        else:
            result = pool.map(incat_similarity,itertools.repeat((cat,chunksize),n_procs))
            cat_results[cat] = np.array(list(itertools.chain(*result)))
    pickle.dump(cat_results,open('cat_results.pkl','wb'))

    cat_results_binned = {}
    for cat in range(251):
        cat_results_binned[cat] = np.histogram(cat_results[cat],bins=bins)[0]

    pickle.dump(cat_results_binned,open('cat_results_binned.pkl','wb'))


    pool.terminate()
